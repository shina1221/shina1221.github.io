---
layout: post
title: 16장
date: 2023-12-08 19:20:23 +0900
category: ML 
use_math: true
---
# 밑바닥부터 시작하는 데이터 과학  

> 문제

ex) 경력, 소득, 유료계정 등록여부 등을 익명화해 저장한 200개의 데이터가 있을 때  
유료 계정 등록 여부 = beta0 + beta1*경력 + beta2*소득 + $\epsilon$  
으로 모델링 했을 때 
![문제](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQWiVSt9BqJGRLfbvg203q0kafF4_hPV4myQw&usqp=CAU)  
예측값들은 다음과 같이 데이터들이 0과 1로만 구분된다.
<br>  
이럴 경우 몇가지 문제가 발생한다.  
1.예측값(x축 prediction)이 명확하게 0 또는 1이거나 확률로 간주할 수 있도록 0 사이의 값으로  
나와야 어떤 클래스에 대한 예측인지 알 수 있다.  
(예를들어 예측값이 0.25면 유로계정 사용자일 확률이 25%라는 의미이다.)  
하지만 선형회귀 모델은 아주 큰 양수값 또는 음수값일 수 있기 때문에 해석이 어렵다.
2.선형회귀분석은 오류값이 변수들과 아무런 상관관계가 없다고 가정한다.  
ex) 경력에 대한 계수가 0.43으로 나왔다면 이는 경력이 많은  
사용자의 경력이 매우 많은 경우 모델은 매우 큰 예측값을 보여줄 것이다.   
하지만 y의 최댓값이 1이므로 아주 큰 예측값과(더불어 아주 많은 경력)은 큰 음의 오류값을 뜻한다.  
즉 이 경우 beta에 대한 추정이 상당히 편향되어 있다.
<br>  
이 대신 출력값이 크면 예측값이 1에 가깝고 작으면 예측값이 0에 가깝도록 설정해볼 수 있다.
<br>  

> 로지스틱 함수

로지스틱 함수는 입력값이 커질수록 출력값이 1에 가까워지고 작을수록 0에 가까워진다.  
*로지스틱 함수 미분  
logistic(x)*(1-logistic(x))  

![수식](https://latex.codecogs.com/svg.image?y_i=f(x_i\beta)&plus;\epsilon_i)  
*f()는 로지스틱 함수  
선형 회귀 분석시 오차의 제곱합(SSE)를 최소화해서 모델 파라미터 beta를 학습했었는데  
해당 모델 파라미터 beta는 결국 데이터의 likelihood를 최대화하는 값이었다.
<br>  
하지만 로지스틱 회귀분석에서는 이 두 가지가 동일하지 않다.  
beta가 주어졌을 때 각 y_i는 $f(x_i,\beta)$의 확률로 1이 되고,  
$1-(f(x_i\beta))$확률로 0이 된다.  
이에 대한 확률밀도함수(probability density function)은 다음과 같다.  
![pdf](https://latex.codecogs.com/svg.image?p(y_i|x_i,\beta)=f(x_i,\beta)^{y_i}-(1-f(x_i\beta))^(1-y_i))  
y가 0이면 식은 이렇게 된다.  
$1-f(x_i\beta)$  
y가 1이면 식은 다음과 같다.    
$f(x_i\beta)$  
따라서 log likelihood를 최대화하는 방법은 다음과 같다.  
log는 단조증가함수이므로 log likelihood를 최대화하는 beta는 동시에 likelihood를 최대화하는  
beta이며 그 반대도 성립한다.  
만약 데이터 포인트끼리 서로 독립이라면 데이터 전체에 대한  
likelihood는 개별 데이터 포인트의 likelihood의 곱이 된다.  
바꿔말하면 데이터 전체에 대한 log likelihood는 개별 데이터 포인트의 log likelihood의 단순 합이다.  
여기에 미적분을 이용하면 gradient를 구할 수 있다.
<br>  

> 모델 적용하기

rescaled_x에 대한 계수  
beta_hat=[-1.90,4.05,-3.87]  
원래 데이터 단위로 변환  
상수, 경력, 소득  
beta_hat_unscaled = [7.61, 1.42, -0.000249]  
해석)다른 모든 것이 동일하다면 경력이 1년 더 추가될 때 logistic 입력값에 1.42가 더해진다.  
또한 다른 모든것이 동일할 때 연봉이 10000씩 추가될 때마다 logistic 입력값에서 2.49가 빠진다.  
행렬의 내적을 구해 dot(beta, x_i)가 0에 가깝다면 특정 변수의 값을 조금만 키워도 예측값에는  
큰 영향을 미칠 수 있다.  
경력이 많을수록 유료계정으로 등록할 가능성이 높고, 다른 모든 것이 동일할 때  
월급이 높을수록 유료계정으로 등록할 가능성이 낮다.
<br>  

> 적합성

precision과 recall을 사용해 결과를 확인한다.  
ex) 정밀도가 93%(유료계정이라고 예측했는데 유료계정 이용자일 확률이 93%이다.)  
재현율이 83%(유료계정 이용자인데 우리가 유료 계정 이용자라고 예측할 확률이 83%이다.)  
나쁘지 않은 결과이다.
<br>  

> SVM 

dot(beta_hat, x_i)의 값이 0이 되는 부분이 두 클래스의 경계면이다.  
파라미터 공간을 두 개의 부분 공간으로 나누는 초평면은 likelihood를 최대화한 
결과의 부산물로 얻을 수 있었던 것이다.  
SVM은 초평면에서 가장 가까운 점까지의 거리를 최대화하는 방식으로 초평면을 찾는다.  
초평면을 찾는것은 복잡한 최적화 이론을 필요로하는데 데이터를 분류할 수 있는  
초평면이 아예 존재하지 않을수도 있다.  
또한 클래스를 완벽하게 가를 수 있는 직선은 존재하지 않을수도 있다.  
이럴 경우 방법 중 하나는 데이터가 선형적으로 구분될 수 있는 조금 더 차원이  
높은 공간으로 현재 공간을 변환하는 것이 있다.  
ex) 데이터 포인트 x를 x**2로 변환  
데이터를 높은 공간으로 변형하면 데이터를 분리할 수 있는 초평면을 찾을 수 있게 된다.  
실제로 이런 작업을 수행할 시 보통 데이터 포인트를 더 높은 차원의 공간으로 직접 매핑하기 보다는  
**보통 커널 함수를 써서 더 높은 차원의 내적을 계산하고 그것으로 초평면을 찾는 커널 트릭을 사용한다.**  

>

참고출처:  
