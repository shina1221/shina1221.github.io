---
layout: post
title: 17장
date: 2023-12-09 19:20:23 +0900
category: ML 
use_math: true
---
# 밑바닥부터 시작하는 데이터 과학  

> 의사결정나무란

이해하기 쉽고, 예측시숫자형 데이터와 범주형 데이터를 동시에 다룰 수 있다.  
또한 특정 변수의 값이 누락되어도 사용할 수 있다.  
하지만 학습ㄷ ㅔ이터에 대해 최적의 의사결정나무를 찾는것은 어렵다.  
의사결정나무는 새로운 데이터에 대한 일반화 성능이 좋지않게 오버피팅되기 쉽다..  
의사결정나무는 범주형 결과를 반환하는 분류나무와 숫자형 결과를 반환하는 회귀나무로 나뉜다.
<br>  

> 엔트로피

의사결정나무를 만들기 위해선 어떤 질문을 던질 것이고, 어떤 순서로 질문할 것인지를 정해야한다.  
이상적으로 예측하려는 대상에 대해 가장 많은 정보를 담고 있는 질문을 고르는 것이 좋다.  
**얼마만큼의 정보를 담고 있는가를 엔트로피라고 한다.**  
ex)데이터셋 S가 있고 각 데이터 포인트 c1,c2,...cn등은 유한개의 클래스 중 하나에 속한다고 한다.  
모든 데이터 포인트가 단 하나의 클래스에 속한다면 사실 불확실성은 전혀없고 엔트로피도 낮다.  
반면 모든 데이터 포인트가 모든 클래스에 고르게 분포해있다면 엔트로피는 높다고 할 수 있다.  
수학적 개념)  
한 데이터 포인트가 클래스 $c_i$클래스에 속할 확률은 $p_i$이다.  
![수식1](https://latex.codecogs.com/svg.image?&space;H(S)=-p_1log_2p1-...-p_nlog_2p_n)  
각 항 $-p_i,log_1p_i$는 항상 0보다 크거나 같고 $p_i$의 값이 0또는 1에 가까울 때 0에 가까워진다.  
![엔트로피]()
모든 $p_i$rk 0혹은 1에 가까우면 (대부분의 데이터 포인트가 하나의 클래스에 속하면)엔트로피는 아주 작을 것이다.  
```python
def entropy(class_probabilities):
    #클래스에 속할 확률을 입력하면 엔트로피를 계산하라.
    return sum(-p*math.log(p, 2)
        for p in class_probabilities if p) #확률이 0인 경우는 제외 
###
입력데이터는 (input, label)쌍으로 주어져있기 때문에, 각 클래스 레이블의 확률은 별도로 계산  
엔트로피를 구할땐 어떤 레이블에 어떤 확률값이 주어졌는지까지 알 필요가 없고 레이블과 무관하게 확률값들만 알면 된다.
###
def class_probabilities(labels):
    total_count = len(labels)
    return [count / total_count for count in Counter(labels).values()]

def data_entropy(labeled_data):
    labels = [label for _. label in labeled_data]
    probabilities = class_probabilities(labels)
    return entropy(probabilities)
```

> 파티션의 엔트로피 

앞서 데이터셋 전체에 대한 엔트로피를 계산했다.  
하지만 의사결정나무의 각 단계는 데이터를 여러개의 파티션으로 분할하기 때문에 데이터가 여러개의 작은 데이터셋으로 나뉜다.  
이에 따라 하나의 데이터셋을 여러개의 파티션으로 나누더라도 데이터셋 전체에 대한 엔트로피를 계산할 수 있는 방법이  
필요해졌다.  
이 때 파티션 하나하나가 낮은 엔트로피를 가지는 경우 전반적인 엔트로피도 낮고, 파티션 각각이 높은 엔트로피를  
가지는 경우에는 전반적인 엔트로피도 높아야 할 것이다.  
ex)동물의 종류가 3개 이상이라면 개, 고양이로 나뉘는 케이스가 아닌 개, 개 이외의 동물들로 나눠서  
개 이외의 동물들로 나뉜 케이스는 집합도 크고 엔트로피도 높게 나올 것이다. 
수학적 개념)  
데이터 S를 $q_1,...,q_m$의 비율을 가지는 파티션 $S_1,...S_m$으로 나누는 경우에 엔트로피는 가중합으로 구한다.  
*가중합: 복수의 데이터를 단순하게 합하는 것이 아닌 각각의 수에 어떤 가중치값을 곱한 뒤 이 곱셈결과들을 합한 것  
![엔트로피 가중합](https://latex.codecogs.com/svg.image?H=q_1H(S_1)&plus;...&plus;q_mH(S_m))  
이 접근법의 한가지 문제는 다양한 값을 가질 수 있는 변수를 사용해 파티션을 나눌 경우  
오버피팅이 되어 엔트로피가 낮아진다는 것이다.  
ex)은행 고객들이 대출을 잘 갚을 수 있을 것인지에 대해 의사결정나무를 만들려할 때  
변수 중 고객들의 주민등록변호가 담겨있는 변수가 있는데 이 변수로 파티션을 나누게 되면 파티션당  
한 명식만 속하게 되어 엔트로피가 모두 0이되지만 이렇게 파티션이 나뉘면 학습데이터 외의 데이터를  
잘 처리할 수 없을 것이다.  
따라서 **의사결정나무를 사용할 때에는 다양한 값을 가질 수 있는 변수들의 경우를 최대한 피하거나,**  
**변수에 속한 값을 적은 수의 버킷으로 나눠 선택 가능한 값의 종류를 줄이는 것이 좋다.**  
<br>
```python
def partition)entropy(subsets):
#subsets는 레이블이 있는 데이터의 list이다. 그에 대한 파티션 엔트로피를 구하라.
#sum(서브셋의 엔트로피*서브셋의 비율)
total_count = sum(len(subset) for subset in subsets)
#서브셋의 엔트로피*서브셋의 비율
return sum(data_entropy)(data_entropy(subset) * len(subset) / total_count for subset in subsets) 
```

> 의사결정나무 만들기 

의사결정나무는 결정노드와 잎노드로 구성된다.  
결정노드는 질문을 주고 답변에 따라 다른 경로로 안내를 해주고, 잎 노드는 예측값이 무엇인지 알려준다.  
나무는 ID3 알고리즘을 기반해서 구축할 수 있다.
ex)레이블이 있는 데이터와 파티션을 나눌 수 있는 변수가 주어졌다고 가정한다.  
-모든 데이터 포인트의 클래스 레입르이 동일하다면, 그 예측값이 해당 클래스 레이블인 잎 노드를 만들고 종료하라.  
-파티션을 나눌 수 있는 변수가 남아있지 않다면(더 이상 물을 수 있는 질문이 없다면) 가장 빈도 수가 높은 클래스 레이블로  
예측하는 잎 노드를 만들고 종료하라.  
-그게 아니라면 각 변수로 데이터의 파티션을 나눠라  
-파티션을 나눴을 때 엔트로피가 가장 낮은 변수를 택하라  
-선택된 변수에 대한 결정 노드를 추가하라  
-남아 있는 변수들로 각 파티션에 대해 위 과정을 반복하라
<br>  
이와같은 방법을 탐욕적(greedy)알고리즘이라고 한다. 왜냐하면 순간마다 최적이라고 생각되는 선택을 하기 때문이다.  
하지만 순간에는 최적의 선택이 아니더라도 나무 전체적으로는 더 좋은 선택인 경우가 있다.  
ID3 알고리즘은 그렇지 못하지만 이해하기 쉽고 구현도 쉬워 의사결정나무에 입문하는데 좋은 길잡이가 된다.  

가장 낮은 엔트로피를 반환하는 파티션을 찾는다.  
엔트로피를 계산한다.  
전체 데이터셋에 대해 엔트로피를 최소화하는 파티션을 찾는다.  

> 랜덤포레스트

오버피팅을 방지할 수 있는 대표적인 방법중 하나로 랜덤포레스트가 있다.  
여러개의 의사결정나무를 만들고 그들의 다수결로 결과를 결정하는 방법이다.  
랜덤하게 나무를 구축하기 위해선 데이터를 bootstrap 할 수 있다.  
전체 데이터가 아닌 bootstrap_sample의 결과물을 각 나무의 입력값으로 넣어 학습하는 것이다.  
그러면 각 나무가 서로 다른 데이터로 구축되기 때문에 랜덤성이 생기게 된다.  
이 대 부가적인 이점은 샘플링되지 않은 데이터를 테스트 데이터로 이용할 수 있다는 것이다.  
즉 성능을 측정하는 방법만 잘 설계한다면 데이터 전체를 학습에 사용해도 된다는 것을 의미한다.  
이 방법을 bootstrap aggregating 혹은 bagging이라고 한다.  
두번째 방법은 파티션을 나누는 변수에 랜덤성을 부여하는 것이다.  
즉 남아있는 모든 변수 중에서 최적의 변수를 선택하는 것인 아닌 변수 중 일부만 선택해 그 일부 중에서 최적의 변수를 선택하는 것이다.  
이런 방법을 광범위하게 앙상블 학습이라고 한다. 성능이 떨어지는(대부분은 bias가 높고 variance가 낮은)여러 모델을  
동시에 활용해서 전체적으로는 성능이 좋은 모델을 구축한다.
<br>  
*앙상블:여러개의 학습 알고리즘을 사용해 더 좋은 성능을 얻는 방법이다.   
*배깅: 앙상블 학습의 기법 중 하나로 여러개의 기계학습 모델을 독립적으로 학습시킨 후 그 결과를 투표 혹은 평균을 통해 종합하는 방법이다.  
배깅은 주로 분류와 회귀 문제에 적용되며 모델의 성능 향상과 과적합의 감소에 효과적이다.  
배깅의 학습 아이디어는 다수의 약한 학습기를 결합해 강한 학습기를 형성하는 것이다.  
![인공지능 & 머신러닝 책갈피](https://wikidocs.net/152756)  
*부스팅: 여러개의 약한 학습기를 결합해 강한 학습기를 만드는데 사용되는 메타 알고리즘이다.  
부스팅의 주요 아이디어는 약한 학습기들이 순차적으로 훈련되면서, 각 단계에서 이전 학습기가 잘못  
예측한 샘플에 대해 새로운 학습기가 더 많이 집중하는 것이다.  
이 과정은 잘못 분류된 샘플에 더 높은 가중치를 부여함으로써 이뤄진다. 이 과정을 통해 각 학습기는 이전 학습기의  
약점을 보완하게 된다.
<br>  
참고출처:  
https://datascienceschool.net/02%20mathematics/02.02%20%EB%B2%A1%ED%84%B0%EC%99%80%20%ED%96%89%EB%A0%AC%EC%9D%98%20%EC%97%B0%EC%82%B0.html  
https://tyami.github.io/machine%20learning/decision-tree-2-ID3/  
https://wikidocs.net/152756
