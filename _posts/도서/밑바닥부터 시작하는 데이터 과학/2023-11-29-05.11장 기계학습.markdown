---
layout: post
title: 11장
date: 2023-11-29 19:20:23 +0900
category: DA 
use_math: true
---
# 밑바닥부터 시작하는 데이터 과학  

> 모델링  

    
모델: 다양한 변수 간의 수학적(혹은 확률적) 관계를 표현한 것이다.  
기계학습: 지도학습과 비지도 학습이 주된 방법이다.  
이 밖에도 데이터의 일부에만 정답이 포함되어 있는 준 지도 학습(supervised learning)이나  
새로 들어오는 데이터를 통해 모델을 끊임없이 조정하는 온라인 학습이 있다.  

> 오버피팅과 언더피팅

오버피팅: 만들어진 모델의 성능이 학습 데이터에서는 좋지만 기존에 관측한 적이 없는  
새로운 데이터에서는 모델 성능이 좋지 못한 경우이다.  
이런 현상은 데이터의 잡음이 모델에 학습되거나, 원하는 결과를 예측해 주는 요소가 아닌  
다른 요소들이 학습되기 때문에 발생한다.  
언더피팅: 모델의 성능이 학습데이터에서도 좋지 않은 경우이다.  
보통 언더피팅이 발생하면 해당 모델은 문제에 적합하지 않다는 것을 의미하며  
이 때에는 새로운 모델을 찾아봐야 한다.  
모델이 너무 복잡하면 오버피팅이 발생하고, 학습 데이터 이외의 데이터에서는 일반적으로 적용할 수 없다.
<br>  
데이터를 나눠 일부 데이터로 모델의 성능을 평가할 때 모델이 학습 데이터에 오버피팅되었다면  
평가 데이터에서 모델의 성능은 좋지 않을 것이다. 평가 데이터에 대한 성능이 좋은 모델은  
오버피팅 되지 않았다고 볼 수 있을 것이다.
<br>  
이런 경우 몇가지 문제가 존재한다.  
첫번째 문제는 학습 데이터와 평가 데이터에 동일한 패턴이 존재한다면  
더 큰 데이터에서 모델의 일반적인 성능이 좋지 않다는 것이다.  
ex) 각 사용자의 주간 활동 내역을 행으로 나타내는 데이터가 주어졌을 때,  
이 경우 대부분의 사용자는 학습데이터와 평가 데이터에 모두 나타날 것이다.  
모델은 속성관의 관계보다는 사용자를 분류하도록 학습될 수도 있다.  
더 큰 문제는 학습 데이터와 평가 데이터로 하나의 모델을 평가할 때가 아니라 여러 모델중에서 하나의 모델을 선택할 때 발생한다.  
이 경우 각각의 모델은 오버피팅되지 않겠지만, 평가 데이터에서 성능이 제일 좋은 모델을 선택하게 된다면  
평가 데이터를 일종의 두 번째 학습 데이터로 사용하는 메타학습의 문제가 발생한다.  
이 때 평가 데이터에서 성능이 제일 좋았던 모델을 동일한 평가 데이터로 평가한다면  
좋은 성능이 나올 수 밖에 없다.  
이런 경우 데이터를 데이터를 세개로 나눠 학습 데이터로 모델을 만들고, 검증데이터(validation set)를  
통해 학습된 여러 모델 중 하나를 선택해 평가 데이터로 최종 모델의 성능을 평가할 수 있다.
<br>  

> 정확도

모델을 평가하는데 무작정 정확도를 쓰지 않는 이유가 있다.  
ex)이 이메일은 과연 스팸 메일인가?, 이 지원자를 채용해야 하는가?, 이 여행객은 테러리스트인가?  
여러 상황에 따라 맞는 기준을 세워야한다.  
//**True, False: 실제값과 예측값이 일치하는지 여부**  
//**Positive, Negative: 예측한게 1(예측하고자 하는 대상)인지 아닌지 여부**  
//앞에는 예측과 실제값의 일치여부, 뒤에는 예측한 결과  
TP(True Positive): 이 메일은 실제로 스팸 메일이며 정확하게 스팸으로 분류  
FP(False Positive): 이 메일은 실제로 스팸 메일이 아니지만 스팸으로 분류  
FN(False Negative): 이 메일은 실제로 스팸 메일이지만 아니라고 분류   
TN(True Negative): 이 메일은 실제로 스팸 메일이아니며 예측한 결과도 스팸이 아니라고 분류  
이런 정보는 혼동행렬을 써서 표현한다.
<br>  
보통 모델의 성능을 평가하기 위해 정밀도(precision)와 재현율(recall)을 사용한다.  
정밀도(precision): 양성이라고 예측된 결과들 중 진짜 양성의 비율 TP/(TP+FP)  
재현율(recall): 실제 양성인 데이터들 중 양성이라고 맞게 예측한 것의 비율 TP/(TP+FN)  
정확도: (TP+TN)/(TP+FP+TN+FN)
f1 score: 정밀도와 재현율의 조화평균으로 항상 정밀도와 재현율 사이의 값을 가진다.  
조화평균:n개의 양수에 대하여 그 역수들을 산술평균한 것의 역수  
예시)두 개의 값이 x₁와 x₂일 때 조화평균은 2/(1/x₁ + 1/x₂)로 계산된다.
<br>  
보통 모델을 선택하기 위해선 정밀도와 재현율의 트레이드오프를 고려해야한다.  
특정 데이터 포인트가 조금이라도 양성일 것 같을 때 모델이 데이터 포인트를 양성이라고 판단한다면  
재현율은 높겠지만 정밀도는 낮을 것이다.  
반면 모델이 데이터가 확실히 양성일 때만 해당 데이터를 양성이라고 판단한다면  
재현율은 낮겠지만 정밀도는 높을 것이다.
<br>  
이는 false positive와 false positive의 트레이드오프로 볼 수도 있다.  
너무 자주 양성이라고 판단하면 false positive가 증가할 것이다.  
반면 자주 음성이라고 판단하면 false negative가 증가할 것이다.  
ex) 백혈병을 유발하는 10가지 위험 요소가 존재하고 더 많은 위험 요소를 보유할 수록  
발병 확률이 높아진다고 한다.  
이런 경우 적어도 1개의 위험 요소를 보유할 때 발병 여부를 예측해보고, 적어도 2개의 위험 요소를 보유할 때  
발병 여부를 예측하는 등 순차적으로 발병 여부를 판단할 수 있다.  
위험 요소의 개수를 증가시키면 발병 확률이 높아지기 때문에 판독 방법의 정밀도는 증가한다.  
하지만 위험 요소의 개수가 증가하면 점점 더 적은 수의 환자가 양성으로 판정되기 때문에  
판독 방법의 재현율은 감소한다.  
이와 같이 위험 요소의 적당한 개수를 정하는 것은 결국 적절한 트레이드오프 수준을 결정하는 문제이다.  

> bias variance 트레이드 오프  

오버피팅 문제는 bias(편향)과 variance(분산)의 트레이드오프로 볼 수도 있다.  
두 수치는 모델을 더 큰 모집단에서 추출한 다양한 학습 데이터로 모델을 다시  
학습시키면 어떤 변화가 발생하는지를 설명해준다.  
과적합된 모델은 거의 모든 학습데이터에서 오류를 범할 것이다. = bias 되어있다.  
하지만 임의의 두 학습 데이터에서는(두 학습 데이터 각각의 평균은 비슷하기 때문에) 비슷한 모델이 만들어질 것이다. =variance는 낮다.  
오버피팅 된 모델은 bias는 낮지만 (임의의 두 학습 데이터에서 만들어진 모델은  
매우 달라지기 때문에) variance는 높아진다.  
**예측값들과 정답이 멀리 떨어져 있으면 편향이 높다고 본다.**  
**예측값들끼리 흩어져있는 상태면 결과의 분산이 높다고 본다.**
<br>  
bias가 높고 variance가 낮다면 언더피팅을 의미한다.  
bias는 낮지만 variance는 높은것은 오버피팅을 의미한다.  
[편향과 분산 참조](https://opentutorials.org/module/3653/22071)  
![편향 분산 트레이드 오프](https://miro.medium.com/v2/resize:fit:720/format:webp/1*BXJW41upPGZOIpEtA1j_0g.png)
<br>  
모델의 편향이 매우 심하다면(학습 데이터에서도 모델의 성능이 좋지 않다면)  
새로운 변수를 추가하는 것도 하나의 해결책이다.  
반면 모델의 분산이 너무 높다면 모델의 변수를 줄이거나 가능하면 더 많은 데이터를  
구해 모델을 다시 학습시키면 된다.
<br>  
모델의 복잡성이 동일한 상태에서 데이터가 많아지면 오버피팅이 어려워진다.  
하지만 데이터의 수가 늘어나도 bias는 줄어들지 않는다.  
만약 데이터의 패턴을 잡아내기에 모델의 특징이 부족하다면 아무리 많은 데이터를  
추가해도 전혀 도움이 되지 않는다.
<br>  
**편향: $(E[\hat{f}(x)]-f(x))^2$**  
예측값이 정답과 얼마다 다른지 볼 수 있는 지표이다.  
E[]=평균  $\hat{f}$=예측값 f(x)=실제값  
**분산: $E[(\hat{f}(x)-E[\hat{f}(x)])^2]$**  
모델에서 예측값들이 흩어져있는 정도이다.  
$\hat{f}$=예측값 $E[\hat{f}(x)]$=예측값의 평균
<br>   
**데이터들이 예측(회귀)모델과 동떨어져있으면 편향이 높다. <-> 모델이 정답과 거의 일치하면 편항이 낮다.**  
**모델이 표현하는 값들끼리는 별로 떨어져 있지 않으면 분산이 낮다.<-> 모델이 내놓는 값들끼리 흩어져있으면 분산이 높다.**  
<br>  
모델이 데이터를 반복 학습하는 횟수가 늘어날수록 모델의 복잡성도 늘어난다.  
이는 훈련용 데이터를 그대로 외우는 방향이기 때문이다.  
따라서 training error는 갈수록 줄어들지만 validation error는 어느 정도까지 줄어들다가  
어느 지점 이후부터는 다시 상승하게 된다.  
모델을 훈련시키는 도중에 validation error(데이터로 모델 평가시 발생하는 오차)가  
최소인 지점에서 훈련을 멈추는 것이 필요하다.
<br>  
오차 
![오차](https://latex.codecogs.com/svg.image?Error(x)=(E[\hat{f}(x)]-f(x))^2&plus;E[(\hat{f}(x)-E[\hat{f}(x)])^2]&plus;\sigma_{e}^2])  
$\sigma_{e}^2$ = (irreducible error)어떻게 해도 줄일 수 없는 오차  

> 특성 추출 및 선택

데이터를 설명하기에 데이터의 특성을 나타내는 모델의 변수가 부족하면 언더피팅이  
변수가 너무 많다면 오버피팅일 발생할 것이다.
<br>  
데이터의 특성은 모델의 모든 입력변수를 의미한다. 
대부분의 특징은 대부분 다음과 같은 세가지 종류로 구분된다.  
-예, 아니오(1,0): 이메일에 비아그라 라는 단어가 포함되어 있는가?
-숫자: d가 몇번 나왔는가?
-별개의 범주에서 선택: 보낸 사람의 이메일 도메인은 어디인가?
<br>  
특징의 종류에 따라 사용할 수 있는 모델이 제한된다.  
나이브베이즈: 예, 아니오로 표현되는 특징  
회귀 분석 모델: 숫자로 표현되는 특징(1,0으로 변환한 가변수 포함)  
의사결정나무: 숫자나 범주로 표현되는 특징
<br>  
수백 차원의 입력 변수를 상황에 따라 차원 축소를 통해 입력변수를 몇몇 중요한 변수로  
축소시키고 더 적은 수의 변수로 데이터의 특성을 나타낼 수도 있다.  
regularization같은 변수의 개수가 늘어날수록 해당 모델을 불리하게 만드는 기법을 사용할 수도 있다.  
[차원축소 참조](https://opentutorials.org/module/3653/22994)
<br>  
차원 축소는 데이터의 양을 줄이는 여러가지 방법이다.  
목적은 여러가지가 있다.  
1.데이터의 양을 줄여 시간복잡도(계산시간)와 공간복잡도(저장하는 변수의 양)을 줄여 더 적은 자원으로 목적을 달성하기 위함이다.  
2.많은 차원의 데이터로 학습스킨 머신러닝 모델은 내부의 파라미터도 매우 복잡하게 형성해 오버피팅되기가 쉽다.  
이렇게 되면 적은 데이터에 대해 불안정한 결과를 얻게된다.  
차원을 줄여 학습시키면 모델이 간단해지고 적은 데이터셋에 대해 안정적인 결과를 얻을 수 있게 된다.  
3.간단한 모델일수록 내부 구조를 이해하기 편하고 모델의 결과를 2차원이나 3차원으로 축소해  
사람이 결과를 알아보기 편하다.  
다만 차원축소는 원본데이터를 축소하는 과정에서 정보를 일부 없애게 된다.  
어떤 목적을 두고 어떤 정보를 얼마나 없애면 좋을지에 대한 고민의 결과가 차원축소 방법이다.  
<br>  
1.feature selection  
forward search, backward search  
forward search에 비해 backward search는 전체집합부터 제외하면서 시작해 backward search가 계산이 더 오래 걸린다.
<br>  
2.feature extraction  
여러 데이터 열을 압축하는 방법을 제안한다.  
정답 레이블을 사용하지 않은 unsupervised 방법과 이를 사용하는 supervised 방법이 존재한다.  
-factor analysis(요인분석): 예를 들어 2차원 데이터를 생성하는데 가장 적합한 1차원의 숨겨진(hidden, latent)변수가  
있다면 어떻게 생겼을까를 유추하는 것이 목적이다.  
-multidimensional scaling(다차원 척도법): 예를 들어 2차원의 데이터 점 각각의  
위치 등은 고려하지 않고 데이터 점 각각의 거리를 보존하며 1차원으로 축소하려면 어덯게 하면 되겠느냐는 것이다.  
-principal component analysis(PCA):예를 들어 2차원 데이터의 정보 손실을 최소화하기 위해선  
어느 방향으로 짜부러트려야 할 것인가를 푸는 문제이다.  
*정보손실: 데이터의 분산(퍼짐 정도)를 의미한다.  
*principal component: 축소이후에도 분산이 가장 큰 방향을 의미한다.
<br>  
-linear discriminant analysis(선형 판별 분석//LDA):지도학습 방법으로 데이터가 분류된  
결과를 훼손하지 않으며 데이터를 1차원으로 축소하려면 어떻게 하면 되겠는지를 고민한다.  
분류가 되었다는 결과를 가지고 계산을 하므로 지도학습이다.
<br>  
참고출처:  
https://opentutorials.org/module/3653/22071  
https://opentutorials.org/module/3653/22994  
https://medium.com/@ivanreznikov/stop-using-the-same-image-in-bias-variance-trade-off-explanation-691997a94a54  
