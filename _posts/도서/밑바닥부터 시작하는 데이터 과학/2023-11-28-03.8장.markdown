---
layout: post
title: 8장
date: 2023-11-28 19:20:23 +0900
category: ML 
use_math: true
---
# 밑바닥부터 시작하는 데이터 과학  

> 8장 

특정 상황에 가장 적합한 모델을 찾는 것은 모델의 오류를 최소화하는 혹은  
우도를 초대화하는것을 의미한다.  
*우도(likelihood):확률 분포의 모수가 어떤 확률변수의 표집값과 일관되는 정도이다.  
구체적으로는 주어진 표집값에 대한 모수의 가능도는 이 모수를 따르는 분포가 주어진 관측값에 대해 부여하는 확률이다.  
그렇다고 우도는 확률과 동일시하면 안된다. 확률은 모수가 정해진 상태에서 데이터가 관측될 가능성을 의미하고, 우도는 데이터가 관측된 상태에서 특정 확률분포에 대한 믿음의 강도를 나타낸다. 즉 데이터가 주어진 모델에 얼마나 잘 
맞아 떨어지는지를 나타낸다.  
ex) 동전을 10번 던져 3번이 앞면이 나왔다면 이 관측값의 가능도는 앞면이 나올 확률이 높은 모델에 대해 높을 것이다.  
*최대우도법: 주어진 데이터를 가장 잘 표현하는(likelihood의 강도가 가장 쎈) 모수를 찾는 방법
<br>  
gradient(경사, 기울기): 편미분 벡터로 함수가 가장 빠르게 증가할 수 있는 방향이다.  
임의의 시작점을 잡은 뒤 gradient를 계산하고 gradient의 방향(함수의 출력값이 가장 많이  
증가하는 방향)으로 조금 이동하는 과정을 여러번 반복한다.
<br>  

> gradient 계산하기

f가 단변수 함수인 경우 점x에서의 미분값은 x가 아주 조금 변했을 때 f(x)의 변화량을 의미한다.  
미분값은 함수 변화율의 극한값이다.  
만약 f가 다변수 함수라면 여러개의 입력 변수 중 하나에 작은 변화가 있을 때 f(x)의 변화량을  
알려주는 편도함수(partial derivative)도 여러개 존재한다.  
i번째 편도함수는 i번째 변수를제외한 다른 모든 입력변수를 고정시켜 계산할 수 있다.  
그 다음에는 일반적인 도함수와 같은 방법으로 gradient의 근사값을 구할 수 있다.
<br>  
#SGD(stochastic gradient descent)
<br>  
참고출처:  
https://ko.wikipedia.org/wiki/%EA%B0%80%EB%8A%A5%EB%8F%84  
https://recipesds.tistory.com/entry/%EC%9A%B0%EB%8F%84%EB%9E%80-%EB%8F%84%EB%8C%80%EC%B2%B4-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-%EC%A0%90-%EC%B6%94%EC%A0%95%EC%A0%84%EC%97%90-%ED%95%98%EB%8A%94-%EC%A4%80%EB%B9%84-%EC%B2%B4%EC%A1%B0-%EA%B0%80%EB%8A%A5%EB%8F%84-Likelihood%EB%9D%BC%EA%B3%A0%EB%8F%84-%EB%B6%88%EB%A6%AC%EB%8A%94-%EC%B9%B4%EC%98%A4%EC%8A%A4%EC%A0%81%EC%9D%B8-%EB%82%A9%EB%93%9D  

