---
layout: post
title: 15장
date: 2023-12-07 19:20:23 +0900
category: ML 
use_math: true
---
# 밑바닥부터 시작하는 데이터 과학  

시간(분)=alpha * beta1 *(친구 수) + beta2 * (근무시간) + beta3 * (박사 학위 취득 여부) + $\epsilon$  
박사학위 취득 여부는 0과 1로 변환해 가변수 취급할 수 있다.

> 모델

다중 회귀분석 모델의 형식  
$y_i = \alpha + \beta_1 * x_{i1} + ... + \beta_k * x_{ik} + \epsilon_i$
<br>  
최소자승법에 대한 몇가지 추가 가정  
1.x의 열은 서로 일차독립 해야한다. 
ex)독립변수 x는 다음처럼 벡터들의 열로 표현할 수 있다. [1(상수항),49(친구 수),4(하루 근무 시간),0(박사 학위 취득 여부)]  
일차독립이란 어떤 벡터도 다른 벡터의 선형 결합으로 만들어질 수 없다는 것이다.  
이 가정이 성립하지 않는다면 베타를 추정할 수 없다.  
ex)A변수와 똑같은 D변수를 추가한다고 할 때 무슨 값을 넣던지간에 A계수에 임의의 값을 더하고  
D계수에서 똑같은 값을 빼면 모델의 예측값은 변하지 않을 것이다.  
이처럼 변수 A의 계수를 계산할 수 없게된다.
<br>  
2.x의 모든 열은 오류 $\epsilon$과 아무 상관관계가 없다.  
이 가정이 위배되면 잘못된 beta가 추정된다.  
ex)  
실제모델  
근무시간이 더 긴 사람은 더 적은 시간을 사이트에서 보낼 것이다.  
친구 수가 많은 사람일수록 근무 시간이 더 길다.
*근무시간과 친구 수는 양의 상관관계를 지닌다고 가정한다.
<br>  
사이트에서 보내는 시간(분)= alpha + beta1*친구수 +beta2*근무시간 + $\epsilon$  
이와 같은 회귀모델에서 오류를 최소화시키면 beta1은 과소평가될 것이다.  
사이트에서 보내는 시간(분) = alpha+beta1*친구수+$\epsilon$
<br>  
만약 실제 모델의 beta1(실제 모델의 오류를 최소화하는 beta1)을 사용하는 단순 회귀 분석 모델로 예측 성능을 평가한다고 할 때  
beta2>0지만 모델에 포함시키지 않아 근무시간이 긴 사용자의 예측값은 너무 작게 계산될 것이다.  
또한 근무 시간과 친구의 수는 양의 상관관계를 지니기 때문에 친구 수가 많은 사용자의 예측값은 너무 작게 계산될 것이고,  
친구수가 적은 사용자의 예측값은 너무 크게 계산될 것이다.
<br>  
**결국 단순 회귀분석 모델의 오류를 줄이기 위해선 추정된 beta1을 줄여야 한다.**  
이는 **오류를 최소화하는 beta1은 실제 값보다 작아진다는 것을 의미한다.**  
결국 단순회귀 분석 모델은 beta1을 과소평가하게 편향된다.  
**일반적으로 이렇게 독립변수와 오류 사이에 상관관계가 존재한다면**  
**최소자승법으로 만들어지는 모델은 편향된 beta를 추정해준다.**
<br>  
오류를 제곱한 값의 합을 최소화해주는 beta찾기 경사하강법 사용(SGD) 183p pass  

> 모델 해석하기

ex) 분 = 30.63 +0.972 친구 수 -1.868 근무 시간 + 0.911 박사 학위 취득 여부  
**모델의 계수는 다른 모든 것이 동일할 때 해당 항목의 영향력을 나타낸다.**  
다른 모든 것이 동일할 때 친구 수가 한 명 증가하면 사용자가 하루 평균 사이트에서 보내는 시간은 1분 증가한다.  
다른 모든 것이 동일할 때 근무 시간이 한 시간 증가하면 사용자가 하루 평균 사이트에서 보내는 시간은 대략 2분가량 감소한다.
<br>  
**다만 이런 해석은 변수간의 관계를 직접적으로 설명해주지 못한다.**  
**예를 들면 친구의 수가 다른 사용자들의 근무 시간은 서로 다를 수도 있다.**  
이 모델은 이런 관계를 잡아내지 못한다.  
이런 문제는 친구 수와 근무 시간을 곱한 새로운 변수로 해결할 수 있다.  
**새로운 변수를 통해 친구의 수가 증가할수록 근무 시간의 계수를 증가시키거나 감소시킬 수 있다.**  
<br>  
혹은 친구 수가 증가할수록 사이트에서 보내는 시간은 어느 일정한 수준까지 증가하고  
그 이후로는 사이트에서 보내는 시간이 감소할수도 있다.  
이런 경우 친구 수를 제곱한 값을 모델의 변수로 사용해 잡아낼 수 있다.
<br>  
**변수가 점점 추가되기 시작하면 각 계수가 유의미한지를 살펴봐야 한다.**  
변수끼리 곱한 값, 변수의 log값, 변수의 제곱값 등 수 많은 새로운 변수를 추가할 수 있기 때문이다.  

> 적합성(goodness of fit)

**회귀 분석 모델에 새로운 변수를 추가하면 R제곱값이 어쩔 수 없이 증가할 수 밖에 없다.**  
단순회귀 분석 모델은 근무시간과 박사 학위 취득 여부가 0인 다중 회귀분석 모델과 동일하다.  
즉 최적의 다중회귀 분석모델은 언제나 단순회귀분석 모델보다 작은 오류를 가지게 된다.  
따라서 **다중 회귀 분석 모델에서는 각 계수의 표준오차를 확인해야 한다.**
<br>  
**계수의 표준오차는 추정된 beta의 계수가 얼마나 확실한지 알려준다.**  
모델은 주어진 데이터에 적합할 수도 있지만 몇몇 독립변수간 상관관계가 있을 경우 이 변수들의 계수는 무의미할 것이다.  
**오차를 측정하기 위해서 각 오류$\epsilon_i$는 독립적이며, 평균은 0이고**  
**표준편차는 $\sigma$인 정규분포의 확률분포라는 가정이 필요하다.**  
이런 가정을 따르면 통계 SW는 선형대수를 이용해 각 계수의 표준오차를 계산해준다.  
**표준오차가 클 수록 해당 계수는 무의미해진다.**
<br>  

> 여담: bootstrap

생성된 표본분포가 주어졌을 때 표본데이터의 중앙값은 분포자체의 중앙값에 대한 추정치로 사용할 수 있다.  
만약 표본 데이터가 모두 20 근처에 위치하고 있다면 중앙값도 20 근처에 있을 것이다.  
만약 표본 데이터의 반은 0 근처에 위치해 있고 나머지 반은 40 근처에 위치해 있다면 추정된 중앙값을 신뢰하기 힘들것이다.
<br>  
새로운 표본을 계속해서 얻을 수 있다면 각 표본의 중앙값을 계산해 해당 값들의 분포를 확인할 수 있다.  
하지만 새로운 분포를 계속해서 얻기는 힘들다.  
이 때 bootstrap기법을 사용한다. 
**bootstrap은 기존 데이터에서 중복이 허용된 재추출을 통해 새로운 데이터의 각 항목을 생성한다.**  
그리고 만들어진 인공 데이터로 중앙값을 계산해 볼 수 있다.  
ex) 현재 책의 예시에서는 각 분포에 대해 표준편차를 계산했다.  
첫번째 bootstrap의 경우 표준편차가 0에 가깝고,  
두번째 bootstrap의 경우 표준편차가 20에 가깝다고 할 때,  
두번째 bootstrap처럼 데이터가 극단적인 경우 데이터를 직접 살펴보면 문제를 쉽게 파악할 수 있지만,  
대부분의 경우 데이터를 보는것만으로는 부족하다.
<br>  

> 계수의 표준오차

계수의 표준오차를 추정할 때에도 bootstrap을 적용할 수 있다.  
주어진 데이터를 통해 수많은 bootstrap 데이터를 생성하고, 각각의 bootstrap 데이터에서 beta를 추정해볼 수 있다.  
ex) **모든 bootstrap의 데이터에서 특정 독립변수 beta_i의 계수가 크게 변하지 않는다면 추정된 계수는 상당히 신뢰할만하다.**  
하지만 계수가 크게 변할 경우 해당 계수는 신뢰하기 어렵다.
<br>  
각 계수의 표준오차를 추정하고나면 추정한 beta가 0인지 같은 가설을 검증해볼 수 있다.  
ex) 귀무가설: beta_i = 0을 검증하기 위해 (잔차의 독립성에 대한 기본적인 가정과 함께)  
다음의 통계치를 계산할 수 있다.  
![통계치](https://latex.codecogs.com/svg.image?t_j=\hat{\beta_j}/\hat{\sigma_j})  
beta_j의 추정치를 표준오차로 나눈 값은 n-k의 자유도를 지닌 t분포를 따른다.  
beta값에 대한 검정통계치 중 0.05 이하가 아닌 변수가 나올 경우  
해당 변수의 계수는 의미가 없을수도 있다는 것을 암시한다.
<br>  
**만약 적어도 beta_j중 하나는 0이 아니다. 혹은 beta1=beta2, beta3=beta4같은**  
**구체적인 가설검증이 필요할 경우 F검증을 사용할 수 있다.**
<br>  

> regularization  

회귀분석 모델의 변수가 너무 많다면 다양한 문제가 발생할 수 있다.  
1.변수가 많아질 수록 모델이 학습 데이터에 오버피팅 될 것이다.  
2.0이 아닌 계수가 많을수록 모델을 해석하기 어려워진다.  
어떤 현상을 설명하는 것이 목표라면 이 경우 수백개의 변수보다는  
n개의 모델로 작은 모델을 만드는 것을 추천한다.
<br>  
**regularization은 beta가 커질수록 해당 모델에 패널티를 주는 방법이다.**  
그리고 오류와 패널티를 동시에 최소화하는 최적의 모델을 만들 수 있다.  
패널티를 강조할수록 값이 큰 계수에 대한 제한이 더 커진다.  
-ridge regression(릿지 회귀)는 beta_i를 제곱한 값의 합에 비례하는 패널티를 부여한다.  
하지만 상수에 대한 패널티는 부여하지 않는다.  
-lasso regression(라쏘 회귀)는 총 계수의 합을 줄여주는 릿지 회귀의 패널티와 다르게  
모든 계수를 최대한으로 0으로 만들어주며, 보다 희소한(sparse) 모델을 학습하게 해준다.  
릿지회귀의 경우 경사하강법으로 학습가능하지만 라쏘회귀는 경사하강법으로  
학습이 불가능하다.  

참고출처:  
