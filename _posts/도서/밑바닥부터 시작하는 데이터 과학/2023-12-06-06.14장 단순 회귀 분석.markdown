---
layout: post
title: 14장
date: 2023-12-06 19:20:23 +0900
category: ML 
use_math: true
---
# 밑바닥부터 시작하는 데이터 과학  

> 모델

![식1](https://latex.codecogs.com/svg.image?y_i=\beta&space;x_i*\alpha&plus;\epsilon_{i})  
![식2](https://latex.codecogs.com/svg.image?y_i)는 사용자 i가 매일 사이트에서 보내는 시간(분)  
![식3](https://latex.codecogs.com/svg.image?x_i)는 사용자 i의 친구 수  
![식4](https://latex.codecogs.com/svg.image?\epsilon_i)는 모델이 고려하지 못하는 다른 요소 때문에 발생하는 오류  
![식5](https://latex.codecogs.com/svg.image?\epsilon^2)은 오차항의 분산이다.
<br>  
알파값과 베타값이 구해졌다고 할 때, 입력 변수 ![식6](https://latex.codecogs.com/svg.image?X_i)에 따라서 결과를 예측할 수 있다.  
실제값인 ![식7](https://latex.codecogs.com/svg.image?y_i)가 있을 때 오류(실제값-예측값)를 구할 수 있다.
<br>  
알고자 하는 것은 데이터 전체에서 발생하는 총 오류값이다.  
하지만 무조건 모든 오류값을 더해선 안된다.  
만약 ![식8](https://latex.codecogs.com/svg.image?x_1)(1번째 x)의 예측값이 너무 높고 ![식9](https://latex.codecogs.com/svg.image?x_2)(2번째 x)의 예측값이 너무 낮다면 오류값이 상쇄되기 때문이다.  
대신 오류의 제곱값을 더해줘야 한다.
<br>  
*최소자승법: sum of squared errors(![식11](https://latex.codecogs.com/svg.image?[SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2]))를  
최소화해 주는 알파와 베타값을 찾는 것이다.  
미분을 사용하면 오류를 최소화하는 알파와 베타를 찾을 수 있다.
<br>  
x와 y가 학습데이터로 주어졌을 때 오류의 제곱값을 최소화해주는 알파와 베타 계산  
베타(회귀식의 기울기, 회귀계수))**beta = correlation(x,y) * standard_deviation(y) / standart_deviation(x)**  
**![식13](https://latex.codecogs.com/svg.image?\beta = \frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}{\sum_{i=1}^{n}(x_i-\overline{x})^2})**  
알파(회귀식의 절편))**alpha = mean(y) - beta * mean(x)**  
식)![식14](https://latex.codecogs.com/svg.image?y_i = \beta x_i*\alpha+\epsilon_i)  
잔차(오차))**![식15](https://latex.codecogs.com/svg.image?\epsilon_i=Y_i-\hat{Y}=Y_i-\beta*X_i-\alpha)**  
오차항의 분산을 잔차의 표본분산으로 추정한다.  
독립변수 x의 평균이 주어지면, 알파(회귀식의 절편)는 종속변수 y의 평균을 예측해 준다.  
베타(회귀계수, 회귀식의 기울기)는 입력변수가 standard_deviation(x)만큼 증가한다면 예측값도 correlation(x,y)*standard_deviation(y)만큼 증가한다는 것을 의미한다.  
상관계수 공식)![상관계수 공식](https://itwiki.kr/images/4/4f/%ED%94%BC%EC%96%B4%EC%8A%A8_%EC%83%81%EA%B4%80%EA%B4%80%EA%B3%84.png)  
표본 표준편차 공식) $\sqrt{\frac{\sum |x-\mu|^2}{n-1}}$  
만약 x와 y가 완벽한 양의 상관관계를 지닌다면, x가 1표준편차 만큼 증가할 때마다 
y도 1표준편차만큼 증가한다.  
반대로 x와 y가 완벽한 음의 상관관계를 지닌다면 x가 증가할 때마다 y는 감소한다.  
상관관계가 0이라면 베타는 0이 될것이고 이는 x가 예측에 영향을 미치지 않는다는 것을 의미한다.  
[회귀모형 참고](https://velog.io/@qsdcfd/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-%EB%8B%A8%EC%88%9C%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95-%EB%AA%A8%ED%98%95%EC%9D%98-%EC%B6%94%EC%A0%95-gvvmc09l)
<br>  
모델이 주어진 데이터에 얼마나 적합한지 알아보는데 결정계수(R 제곱값)을 본다.  
![상관계수r](https://wikimedia.org/api/rest_v1/media/math/render/svg/ee1e03b44aabd2904cca430279faad515c617891) 
해당 식의 제곱값이다.  
**결정계수는 종속변수의 총 변화량 중 모델이 잡아낼 수 있는 변화량의 비율을 의미한다.**  
모델이 잡아낼 수 있는 변화량의 비율(결정계수)= 1-모델이 잡아낼 수 없는 변화량의 비율  
결정계수=SSR/SST  
모델이 잡아내지 못하는 변화량)SSE(sum of squared errors)/SST(total sum of squares)  
SSE = $\sum_{i=1}^{n} (y_{i} - \hat{y})^2$  
SST = $\sum_{i=1}^{n} (y_{i} - \overline{y})^2$  
SSR = $\sum_{i=1}^{n} (\overline{y} - \overline{y})^2$  
//de_mean=sum(v ** 2 for v in (y_i - mean(y)))  
//책에 de_mean에 대한 설명이 없어서 y_i - mean(y)로 이해
<br>  
이외에도 모든 예측값을 항상 mean(y)로 계산해주는 또 다른 모델(알파=mean(y), 베타=0)을 만들수도 있다.  
이 경우 오류를 제곱한 값의 총합은 항상 변화량을 제곱한 값의 총합과 같고 결정계수 값은 0이 될 것이다.  
즉 오류의 제곱 값을 최소화하는 모델은 항상 mean(y)로 예측하는 것과 별다른 차이가 없다는 것을 의미한다.
<br>  
최소자승법 모델의 성능은 적어도 평균을 예측하는 모델의 성능만큼 좋아야 한다.  
즉 오류를 제곱한 값의 총합은 아무리 커봐야 변화량을 제곱한 값의 총합과 동일할 것이다.  
이 경우 R 제곱값은 0이 될 것이다.  
오류를 제곱한 값의 총합은 최소한 0이기 때문에 R 제곱값의 최대는 1이다.  
R 제곱값이 클수록 모델이 데이터에 더 적합하다는 것을 의미한다.  
계산된 R 제곱값이 0.329일 때 이는 만들어진 모델이 어느정도 데이터에 적합하나  
다른 중요한 요소가 있다는 점을 시사한다.
<br>  
책의 178p 경사하강법을 사용하는 부분은 pass
<br>  
> 최대우도추정법 

최소자승법을 사용한 이유는 최대우도추정법(maximum likelihood estimation, MLE) 때문이다.  
ex)임의의 파라미터 theta에 의존하는 분포에서 표본 데이터가 주어졌을 때  
$L(v_1,...,v_n|\theta)$  
theta를 모르기 때문에 표본 데이터가 주어졌을 때 theta가 발생할 likelihood(우도)로 바꿔 생각해볼 수 있다.  
$L(\theta|v_1,...,v_n)$  
이 경우 가장 적절한 theta는 likelihood 함수를 최대화해주는 값이다.  
즉 **관측된 데이터가 발생할 경우를 가장 높게 만들어주는 값이라는 의미**이다.  
확률질량함수 대신 확률밀도함수를 사용하는 연속형 분포에도 이를 동일하게 적용할 수 있다.
<br>  
대부분의 회귀 분석에서는 오류를 평균이 0이고 표준편차가 $\sigma$인 정규분포를 따른다고 본다.  
이 경우 (x_i, y_i)가 관측될 likelihood는 다음과 같다.  
$f(\alpha,\beta|x_i,y_i,\sigma)=\frac{1}{\sqrt{2\pi}\sigma} exp(-(y_i-\alpha-\beta x_i)^2/2\sigma^2)$  
전체 데이터에 대한 likelihood는 각 데이터의 likelihood를 모두 곱한 값이다.  
그리고 오류의 제곱값을 최소화하는 알파와 베타가 계싼되는 지점이 likelihood가 최대화되는 지점이다.  
이런 가정을 따르면 오류의 제곱값을 최소화하는 것은 관측된 데이터가 발생할 likelihood를 최대화하는 것과 동일하다.  

참고출처:  
https://ko.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/variance-standard-deviation-population/a/calculating-standard-deviation-step-by-step  
https://blog.naver.com/qtdbsdk/220018979312  
https://velog.io/@qsdcfd/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-%EB%8B%A8%EC%88%9C%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95-%EB%AA%A8%ED%98%95%EC%9D%98-%EC%B6%94%EC%A0%95-gvvmc09l  
https://recipesds.tistory.com/entry/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-%EA%B2%B0%EA%B3%BC%EC%9D%98-%ED%95%B4%EC%84%9D%EA%B3%BC-R%C2%B2%EC%84%A4%EB%AA%85%EB%A0%A5%EC%9D%98-%EC%9D%98%EB%AF%B8-%EA%B7%B8%EB%A6%AC%EA%B3%A0-R%EC%9D%80-%EC%83%81%EA%B4%80%EA%B3%84%EC%88%98%EC%9D%98-%EC%A0%9C%EA%B3%B1-%EC%9D%91  
https://acdongpgm.tistory.com/70  
https://bpapa.tistory.com/30  