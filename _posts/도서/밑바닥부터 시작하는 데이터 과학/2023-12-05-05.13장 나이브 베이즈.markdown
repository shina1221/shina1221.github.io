---
layout: post
title: 13장
date: 2023-12-05 19:20:23 +0900
category: ML 
use_math: true
---
# 밑바닥부터 시작하는 데이터 과학  

> 모델

[4장~6장 참고](https://shina1221.github.io/statistics/2023/11/24/01.4%EC%9E%A5~6%EC%9E%A5.html)  
[베이즈 정리 이해 참고](https://smwgood.tistory.com/15)  

> 바보 스팸 필터

메시지에 비아그라라는 단어가 포함되어 있는 경우 V  
스팸 메일인 경우 S  
not 논리기호가 없어 !로 대체  
P(S|V) = [P(S|V)P(S)]/[P(V|S)P(S)+P(V|!S)P(!S)]  
분자: 메시지가 스팸이면서 비아그라 단어를 포함하고 있을 확률  
분모: 메시지에 비아그라라는 단어가 포함되어있을 확률  
현재 식은 비아그라라는 단어가 포함된 모든 메시지 중에서 스팸메일의 비율을 의미한다.  
만약 메시지가 스팸일 확률과 스팸이 아닐 확률이 동일하다면 (P(S)=P(!S)=0)  
P(S|V)=P(V|S)+P(V|!S)  
ex) 스팸 메시지 중 50%, 스팸이 아닌 메시지 중 1%만이 비아그라라는 단어를 포함하고 있을 경우  
비아그라라는 단어를 포함하고 있는 메시지가 스팸일 확률  
0.5/0.5+0.01=0.98%  

> 조금 더 똑똑한 스팸 필터

다음의 것들이 주어졌다고 해본다.  
단어 $W_i$가 포함되는 경우: $X_i$  
스팸 메시지에 i번째 단어가 포함되어 있는 확률인 P($X_i$|S)  
스팸이 아닌 메시지에 i번째 단어가 포함되어 있는 확률 P($X_i$|!S)
<br>  
나이브(단순한) 베이즈의 핵심  
**메시지가 스팸이냐 아니냐가 주어졌다는 조건하에 각 단어의 존재(혹은 부재)는 서로 조건부 독립이다.**  
라는 (말도 안되는)가정에 기반을 둔다.  
직관적으로 어떤 스팸 메시지가 비아그라 라는 단어를 포함하고 있다는 점이  
같은 메시지가 롤렉스 라는 단어를 포함하고 있는지를 판단하는데 도움을 주지 않는다는 것이다.  
$P(X_1=x_1,...,x_n=x_n|S)=P(X_1)=x_1|S)*...*P(X_n=x_n|S)$  
<br>  
나이브베이즈는 극단적인 가정을 한다.  
ex)사전에 수록된 단어가 비아그라와 롤렉스 뿐이며, 모든 스팸 메시지의 반은 값싼  
비아그라에 대한 메시지이고 나머지 스팸 메시지는 정품 롤렉스에 대한 메시지라고 할 때  
나이브 베이즈는 스팸 메시지에 메시지에 ㅂ아그라와 롤렉스 라는 단어가 포함될 확률은 다음과 같이 주장한다.  
$P(X_1=1,X_2=1|S)=P(X_1=1|S)P(X_2|S)=0.5*0.5=0.25$  
현실에서는 비아그라와 롤렉스가 동시에 등장하지 않는다는 가정이 없었기 때문이다.
<br>  
말도 안되는 가정이지만 성능은 상당히 뛰어나다.  
비아그라라 라는 단어만으로 스팸을 걸러 내는 필터에서도 사용된 베이즈 정리를 통해  
메시지가 스팸일 확률은 다음과 같다.  
P(S|X=X)=P(X=x|S)/[P(X=x|S)+P(X=x|!S)]  
나이브 베이즈의 가정을 따르면 각 단어가 메시지에 포함될 확률값을 모두 곱하면 값을 구할 수 있다.  
단 실제 구현시 확률값을 끊임없이 곱하는 것은 지양한다.  
컴퓨터가 0에 가까운 부동소수점을 제대로 처리하지 못해 언더플로가 발생하기 때문이다.  
log(ab)=log(a)+log(b)이고 exp(log(x))=x이다.  
부동 소수점 문제를 피하기 위해 $p_1*...*p_n$은 $exp(log(p_1)+...+log(p_n)$으로 계산한다.
<br>  
스팸이나 스팸이 아닌 메시지에 단어 $w_i$가 포함될 확률인 $P(X_i|S)$와 $P(X_i|!S)$의 값을 추정하는 방법은 다음과 같다.  
$P(X_i|S)$를 추정하는 방법 중 가장 간단하게 스팸 메시지 중 $w_i$가 포함되어 있는  
메시지의 비율을 사용할 수 있다.  
하지만 이런 경우 주어진 학습 데이터에 데이터라는 단어는 스팸이 아닌 메시지에만  
포함되어 있다고 할 경우 P(데이터|S)=0이 될 것이다.  
따라서 나이브 베이즈 분류기는 데이터 라는 단어가 들어간 메시지를 항상 스팸 메시지가 아니라고 예측할 것이다.  
값싼 비아그라와 정품 롤렉스에 대한 데이터라는 메시지도 스팸이 아니라고 할 것이다.  
이런 문제를 해결하기 위해선 일종의 **smoothing(평활화)**가 필요하다.  
*평활화: 매끄럽게함으로써 데이터에서 중요하지 않은 잡음 등을 제거하고 그 패턴을 알아내는 방법이다.
<br>  
smoothing을 위해 가짜 빈도수 k를 결정하고 스팸 메시지에서 i번째 단어가 나올 때 확률을 다음과 같이 추정할 수 있다.  
P($X_i$|S)=(k+$w_i$를 포함하고 있는 스팸 수) / (2k+스팸 수)  
스팸 메시지에서 i번째 단어가 나올 확률을 계산할 때 해당 단어가 포함된 스팸과 포함되지 않은  
스팸이 이미 각각 k번씩 나왔다고 가정한다.  
ex) 데이터 라는 단어는 98개의 스팸 문서에서 단 한 번도 나타나지 않았지만,  
k가 1이라면 P(데이터|S)를 1/100=0.01로 계산할 수 있다.  
즉 데이터라는 단어가 포함된 메시지가 스팸 메시지일 확률을 0이 아닌 다른 확률 값으로  
설정할 수 있게 해준다.  
<br>  
참고출처:  
https://en.wikipedia.org/wiki/Smoothing  
https://kr.mathworks.com/help/signal/ug/signal-smoothing.html  
