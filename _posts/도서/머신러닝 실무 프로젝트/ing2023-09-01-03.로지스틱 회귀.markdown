---
layout: post
title: 로지스틱 회귀분석
date: 2023-09-01 19:20:23 +0900
category: ML 
use_math: true
---
# 머신러닝 실무 프로젝트
      
> 로지스틱 회귀분석

로지스틱 회귀는 선형분리 가능한 대상을 분리하는 알고리즘이다.  
-출력과 별도로, **출력값에 해당하는 클래스에 속할 확률을 계산할 수 있다.**  
-온라인 학습과 배치 학습이 모두 가능하다.  
-예측성능은 보통이며 학습 속도가 빠르다.  
**-과적합을 방지하는 규제항이 추가되어 있다.**  
  
> 로지스틱 회귀의 구조  

퍼셉트론과 다르게 **활성화 함수가 시그모이드 함수**이고, **손실함수가 교차 엔트로피 오차 함수이고**   
**규제항**이 있어 과적합을 방지할 수 있다. 또한 온라인 학습과 배치 학습에 모두 적용가능하다.  

![시그모이드](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Error_Function.svg/320px-Error_Function.svg.png)  
입력이 0일 땐 함수값이 0.5이고 입력이 작아질수록 0에 함수값은 0에 가까워진다.  
**시그모이드 함수를 일반화한 것을 로지스틱 함수라고 한다.**  
시그모이드 함수를 거친 값은 어떤 클래스에 속할 확률이 되며, 0.5를 기준으로  
클래스에 속하는지를 판단한다.  
0.5라는 기준을 하이퍼파라미터로 간주해 원하는 성능에 따라 조절하는 경우도 있다.  
  
```python  
def sigmoid(x):
    return 1/(1+np.exp(-x))
#출력 y는 다음과 같이 나타낼 수 있다.  
y = sigmoid(np.dot(w,x))     
```  
N개의 데이터에 대해 출력을 y, 정답 레이블(맞으면 1 아니면 0)을 t, 로그밑을 자연상수로  
하면 이진 분류에 적용되는 교차 엔트로피 오차함수의 식은 다음과 같다.  
$E= -\sum_{n=1}^{N}t_{n}log y_{n} + (1-t_n)log(1-y_n)$  

이 손실값은 정답(t=1)을 맞췄을 때 $log y_n$이 된다. 또한 가중치와 입력의 곱셈합  
(퍼셉트론으로 보면 np.dot(w,x))이 0보다 작으면 손실값이 급격하게 커진다.  
반대로 0보다 크면 손실값이 작아진다. 따라서 가중치와 입력의 곱셈합을 크게 하는  
방향으로 작용한다. 오답일 때는 손실값이 $log(1-y_n)$이 된다.
![참고링크1](https://gihyo.jp/dev/serial/01/machine-learning/0018)  
  
이진분류에 쓰이는 교차 엔트로피 오차함수의 코드는 다음과 같다.  
```python
#1e-15는 0.000 000 000 000 001
#np.clip은 전달받은 숫자가 최소값과 최대값 사이인지 체크한다.  
#최소값보다 작을땐 최소값을 return하고 최대값보다 클 경우는 최대값을 return한다.
#np.min(value_Array, min, max)

def cross_entropy_error(y,t, eps=1e-15):
    y_clipped = np.clip(y, eps, 1-eps)  
    return-1 * (sum(t * np.log(y_clipped))+
    (1-t) * np.log(1-y_clipped))

#np.clip(y, eps, 1-eps)는 y값이 0혹은 1이 되지 않도록 eps라는 작은 값을 더해준다.  
#이는 로그값이 음의 무한대가 되는것을 피하기 위함이다. 
#numpy.array배열을 사용한다는 전제하에 np.clip을 사용했다.  
#만약 데이터가 하나라면 max(min(y, 1-eps), eps)와 같다.  
```  
  
> 규제화  

학습 수행시 페널티를 부여해 결정 경계를 매끄럽게 한다.  
입력 데이터에 대한 최적화를 할 때 규제항을 추가해주면 이미 노출된 학습 데이터의 영향을  
과도하게 받지 않는다. 즉 간단한 모델을 유지하도록 보정하는 역할을 한다.  
규제항을 쓰면 과적합을 억제하고 일반화 성능을 확보할 수 있다.  
규제항을 추가하면 목적 함수를 다음과 같이 나타낼 수 있다.  
**목적함수 = 모든 데이터에 대한 손실함수의 값의 합 + 규제항**  
목적함수는 최솟값을 갖게 하는 파라미터를 추정하는 방법으로 로지스틱 회귀 학습이  
이뤄진다. 퍼셉트론과 마찬가지로 확률적 경사하강법을 사용해 최적화한다.    

> 규제화  
L1:   
L2:  
Elastic Net:  

참고 출처:  
https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%ED%95%A8%EC%88%98  

