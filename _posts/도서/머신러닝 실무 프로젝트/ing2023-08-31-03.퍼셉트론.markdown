---
layout: post
title: 퍼셉트론    
date: 2023-08-30 19:20:23 +0900
category: ML 
---
# 머신러닝 실무 프로젝트
      
> 퍼셉트론  

입력벡터와 학습한 가중치 벡터를 곱한 값을 합한 값이 0 이상일 때 클래스 1로  
0 미만일 땐 클래스 2로 분류하는 간단한 알고리즘이다. 이런 퍼셉트론을 여러 층  
쌓으면 신경망이 된다.  

> 퍼셉트론의 특징 
- 온라인 학습 방식을 취한다.  
- 예측 성능은 보통이지만 학습이 빠르다.  
- 과적합되기 쉽다.  
- 선형 분리 가능한 문제만 풀 수 있다.(=이항분류만 가능하다.)  

선형분리할 때 데이터를 두 클래스로 분리할 수 있는 직선을 초평면이라고 한다.  
2차원에서는 직선이지만 3차원이나 고차원에서는 평면이 되므로 초평면이라고 한다.  
선형으로 나눌 수 없는 배타적 논리합(XOR)같은 케이스에는 부적합하다.  
  
> 퍼셉트론의 구조   

퍼셉트론은 입력과 특징의 중요도를 나타내는 가중치를 곱한 뒤 그 곱의 합을 계산한다.  
입력을 리스트 x, 가중치를 리스트 w라고 하면 다음과 같이 나타낼 수 있다.  
```python
sum = b + w[0] * x[0] + w[1] *x[1]
```  
b는 편향이라고 하며 입력에 곱해지지 않는 특수한 가중치다.  
곱셈합을 구할 땐 넘파이의 numpy.dot 함수를 활용할 수 있다.  
```python
import numpy as np 
w = np.array([2,3])
x = np.array([4,2])
sum = np.dot(w,x)
```  
이후 곱셈합 값 sum이 양수인지 음수인지에 따라 클래스를 판별한다.  
```python
if sum >=0:
    return 1 
if sum <0:
    return -1
```  

> 파라미터 w값 알아내기  

손실함수(오차함수)를 통해 실제값과 예측값이 얼마나 다른지, 예측모델이 얼마나 좋은지  
측정할 수 있다.  
손실함수 = (실제값-예측값)^2  

가중치 벡터를 w, 입력 벡터를 x, 정답 레이블일 t(1 혹은 -1)라고 했을 때  
퍼셉트론의 손실함수는 max(0, -twx)로 정의되는 **힌지 손실(퍼셉트론 규준)**을 사용한다.  
*SVM에서도 힌지 손실을 사용하는데 이는 max(0,1, -twx)로 조금 다르다.  
 퍼셉트론과 svm 둘은 가로축과의 교점이 서로 다르다.  
     
힌지 손실을 사용하면 값이 0보다 작을 때(잘못 분류했을 때) 손실이 커지고,  
제대로 분류했을 땐 0이 된다.  
예측값이 틀릴 수록 손실값은 선형으로 증가한다.  
```python
import numpy as np
def perceptron_hinge_loss(w,x,t):
    loss=0
    for (input, label) in zip(x,t):
        v = label * np.dot(w, input)
        loss += max(0, -v)
    return loss  
```
잘못 분류된 데이터 수가 작아지는 가중치를 찾으면 이 전체 데이터에 대한 손실값의 합  
또한 최소가 된다.  
  
손실함수를 일반화해 어떤 모델이 데이터에 부합하는지를 나타내는 목적함수(평가함수)로 만든다.  
목적함수 = 모든 데이터에 대한 손실 함수값의 합  
**목적 함수가 최소가 될 때를 최적의 상태라고 할 수 있다.**(잘못 분류된 데이터가  
가장 적다.) **이런 상태가 되는 가중치 벡터 w를 구하는 과정을 모델 학습이라고 한다.**   
  
파라미터(가중치 벡터 w)를 최적화하기 위해선 확률적 경사하강법(SGD)를 많이 사용한다.  
SGD는 경사가 가장 심한 쪽으로 조금씩 나아가며 파라미터를 수정한다.  
이렇게 해서 목적 함수의 값이 가장 작은 지점에 도달하면 그 지점의 파라미터 값을 최적의  
값으로 삼는다.(=해가 수렴했다.)  
이 때 파라미터를 한 번에 어느 정도씩 수정할지 결정하는 하이퍼파라미터를 학습률이라고 한다.  
파라미터의 수정 폭은 학습률 경사다. 학습률 값이 너무 크면 수렴이 빨라질 수도 있지만,  
자칫하면 골짜기를 지나쳐 수렴이 아예 되지 않는 경우도 발생한다.  
또한 학습률이 너무 낮으면 수렴할 때까지 반복 횟수가 너무 많이져 학습 속도가 느려진다.  
  
퍼셉트론의 예상값은 곱셈합을 계단함수에 통과시킨 것과 같다.  
![계단함수](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/220px-Dirac_distribution_CDF.svg.png)  

계단함수는 입력갑슬 -1 혹은 +1로 바꿔준다.  
이와 같이 **출력값을 비선형 변환하는 함수를 활성화 함수라고 한다.**    

참고 출처:  
https://ko.wikipedia.org/wiki/%EB%8B%A8%EC%9C%84_%EA%B3%84%EB%8B%A8_%ED%95%A8%EC%88%98